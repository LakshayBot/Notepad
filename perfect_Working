from scrapling.fetchers import StealthyFetcher, AsyncStealthySession, DynamicFetcher
from scrapling.parser import Selector
import json
import asyncio
import re

async def get_video_urls_async(post_urls, max_tabs=5):
    """
    Extract video URLs using AsyncStealthySession with tab-based concurrency
    """
    video_urls = {}
    
    async with AsyncStealthySession(headless=True, solve_cloudflare=True, timeout=45) as session:
        # Create semaphore to limit concurrent tabs
        semaphore = asyncio.Semaphore(max_tabs)
        
        # Create tasks for all URLs with semaphore control
        tasks = []
        for url in post_urls:
            tasks.append(fetch_single_video_url_with_semaphore(session, url, semaphore))
        
        # Execute all tasks concurrently
        print(f"Processing {len(post_urls)} video URLs with {max_tabs} concurrent tabs...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for i, result in enumerate(results):
            url = post_urls[i]
            if isinstance(result, Exception):
                print(f"Error fetching video for {url}: {result}")
                video_urls[url] = ''
            else:
                video_urls[url] = result
                if result:
                    print(f"Found video URL for {url}: {result}")
                else:
                    print(f"No video URL found for {url}")
    
    return video_urls

async def fetch_single_video_url_with_dynamic_fallback(session, url):
    """
    Fetch video URL with fallback to DynamicFetcher for stubborn pages
    """
    try:
        # First try with the existing session
        return await fetch_single_video_url(session, url)
    except Exception as e:
        print(f"Session fetch failed for {url}, trying DynamicFetcher: {e}")
        try:
            # Fallback to DynamicFetcher for problematic pages
            page = DynamicFetcher.fetch(url, headless=True, solve_cloudflare=True, wait_until='networkidle', timeout=60)
            html = page.body
            parsed = Selector(html)
            
            # Extract post ID from URL for pattern matching
            post_id_match = re.search(r'/post/([a-f0-9]+)\.html', url)
            if not post_id_match:
                return ''
            post_id = post_id_match.group(1)
            
            # Look for the player_el video element
            video_url = ''
            
            # Check for video element with id="player_el"
            player_el = parsed.css_first('#player_el')
            if player_el:
                src = player_el.attrib.get('src', '')
                if src and '.vid' in src:
                    video_url = src
                    # If relative URL, make it absolute
                    if video_url.startswith('/'):
                        video_url = 'https://sxyprn.com' + video_url
                    return video_url
            
            # Fallback: Check for any video elements with class containing "player_el"
            if not video_url:
                videos = parsed.css('video[class*="player_el"]')
                for video in videos:
                    src = video.attrib.get('src', '')
                    if src and '.vid' in src:
                        video_url = src
                        # If relative URL, make it absolute
                        if video_url.startswith('/'):
                            video_url = 'https://sxyprn.com' + video_url
                        break
            
            # Additional fallback: Look for any video with .vid src and matching post ID
            if not video_url:
                videos = parsed.css('video')
                for video in videos:
                    src = video.attrib.get('src', '')
                    if src and '.vid' in src and post_id in src:
                        video_url = src
                        # If relative URL, make it absolute
                        if video_url.startswith('/'):
                            video_url = 'https://sxyprn.com' + video_url
                        break
            
            return video_url
            
        except Exception as dynamic_e:
            print(f"DynamicFetcher also failed for {url}: {dynamic_e}")
            return ''

async def fetch_single_video_url_with_semaphore(session, url, semaphore):
    """
    Wrapper to control concurrency with semaphore
    """
    async with semaphore:
        return await fetch_single_video_url_with_dynamic_fallback(session, url)

async def fetch_single_video_url(session, url):
    """
    Fetch video URL for a single post using the existing session with retry logic
    """
    max_retries = 2
    for attempt in range(max_retries + 1):
        try:
            # Extract post ID from URL for pattern matching
            post_id_match = re.search(r'/post/([a-f0-9]+)\.html', url)
            if not post_id_match:
                return ''
            post_id = post_id_match.group(1)
            
            # Add progressive delay to avoid overwhelming the server
            await asyncio.sleep(0.3 * (attempt + 1))
            
            # Fetch the post page using the existing session
            page = await session.fetch(url)
            html = page.body
            parsed = Selector(html)
            
            # Look for the player_el video element
            video_url = ''
            
            # Check for video element with id="player_el"
            player_el = parsed.css_first('#player_el')
            if player_el:
                src = player_el.attrib.get('src', '')
                if src and '.vid' in src:
                    video_url = src
                    # If relative URL, make it absolute
                    if video_url.startswith('/'):
                        video_url = 'https://sxyprn.com' + video_url
                    return video_url
            
            # Fallback: Check for any video elements with class containing "player_el"
            if not video_url:
                videos = parsed.css('video[class*="player_el"]')
                for video in videos:
                    src = video.attrib.get('src', '')
                    if src and '.vid' in src:
                        video_url = src
                        # If relative URL, make it absolute
                        if video_url.startswith('/'):
                            video_url = 'https://sxyprn.com' + video_url
                        break
            
            # Additional fallback: Look for any video with .vid src and matching post ID
            if not video_url:
                videos = parsed.css('video')
                for video in videos:
                    src = video.attrib.get('src', '')
                    if src and '.vid' in src and post_id in src:
                        video_url = src
                        # If relative URL, make it absolute
                        if video_url.startswith('/'):
                            video_url = 'https://sxyprn.com' + video_url
                        break
            
            return video_url
            
        except Exception as e:
            if attempt == max_retries:
                print(f"Failed to fetch video URL after {max_retries + 1} attempts for {url}: {e}")
                return ''
            else:
                print(f"Attempt {attempt + 1} failed for {url}, retrying... Error: {e}")
                await asyncio.sleep(2 * (attempt + 1))  # Progressive backoff
                continue

async def fetch_and_parse_pages_async(page_urls, max_tabs=5):
    """
    Fetch and parse multiple pages concurrently using tab-based approach
    """
    all_data = []
    
    async with AsyncStealthySession(headless=True, solve_cloudflare=True, timeout=45) as session:
        # Create semaphore to limit concurrent tabs
        semaphore = asyncio.Semaphore(max_tabs)
        
        # Create tasks for all pages with semaphore control
        tasks = []
        for url in page_urls:
            tasks.append(fetch_single_page_with_semaphore(session, url, semaphore))
        
        # Execute all tasks concurrently
        print(f"Processing {len(page_urls)} pages with {max_tabs} concurrent tabs...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for i, result in enumerate(results):
            url = page_urls[i]
            if isinstance(result, Exception):
                print(f"Error fetching {url}: {result}")
                continue
            else:
                all_data.extend(result)
                print(f"Fetched {len(result)} posts from {url}")
    
    return all_data

async def get_video_urls_with_session(session, post_urls, max_tabs=5):
    """
    Extract video URLs using existing session
    """
    video_urls = {}
    
    # Create semaphore to limit concurrent tabs
    semaphore = asyncio.Semaphore(max_tabs)
    
    # Create tasks for all URLs with semaphore control
    tasks = []
    for url in post_urls:
        tasks.append(fetch_single_video_url_with_semaphore(session, url, semaphore))
    
    # Execute all tasks concurrently
    print(f"Processing {len(post_urls)} video URLs with {max_tabs} concurrent tabs...")
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Process results
    for i, result in enumerate(results):
        url = post_urls[i]
        if isinstance(result, Exception):
            print(f"Error fetching video for {url}: {result}")
            video_urls[url] = ''
        else:
            video_urls[url] = result
            if result:
                print(f"Found video URL for {url}: {result}")
            else:
                print(f"No video URL found for {url}")

    return video_urls

async def fetch_all_data_with_videos(page_urls, initial_data, max_tabs=5):
    """
    Fetch all pages and video URLs using a single session
    """
    all_data = list(initial_data)  # Start with initial data
    
    async with AsyncStealthySession(headless=True, solve_cloudflare=True, timeout=45) as session:
        # Fetch additional pages if any
        if page_urls:
            additional_data = []
            semaphore = asyncio.Semaphore(max_tabs)
            
            tasks = []
            for url in page_urls:
                tasks.append(fetch_single_page_with_semaphore(session, url, semaphore))
            
            print(f"Processing {len(page_urls)} pages with {max_tabs} concurrent tabs...")
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                url = page_urls[i]
                if isinstance(result, Exception):
                    print(f"Error fetching {url}: {result}")
                    continue
                else:
                    additional_data.extend(result)
                    print(f"Fetched {len(result)} posts from {url}")
            
            all_data.extend(additional_data)
        
        # Now get video URLs using the same session
        post_urls = [d['post_url'] for d in all_data if d['post_url']]
        if post_urls:
            video_urls = await get_video_urls_with_session(session, post_urls, max_tabs)
            for d in all_data:
                d['video_url'] = video_urls.get(d['post_url'], '')
    
    return all_data

async def fetch_single_page_with_semaphore(session, url, semaphore):
    """
    Wrapper to control page fetching concurrency with semaphore
    """
    async with semaphore:
        return await fetch_single_page(session, url)

async def fetch_single_page(session, url):
    """
    Fetch and parse a single page using the existing session with retry logic
    """
    max_retries = 2
    for attempt in range(max_retries + 1):
        try:
            # Add progressive delay to avoid overwhelming the server
            await asyncio.sleep(0.4 * (attempt + 1))
            page = await session.fetch(url)
            html = page.body
            return parse_posts(html)
        except Exception as e:
            if attempt == max_retries:
                print(f"Failed to fetch page after {max_retries + 1} attempts for {url}: {e}")
                raise e
            else:
                print(f"Attempt {attempt + 1} failed for {url}, retrying... Error: {e}")
                await asyncio.sleep(3 * (attempt + 1))  # Progressive backoff
                continue

def parse_posts(html_content):
    parsed_page = Selector(html_content)
    posts = parsed_page.css('.post_el_small .post_control')
    data = []
    for post in posts:
        try:
            a_tag = post.css_first('a.post_time')
            if not a_tag:
                continue
            title = a_tag.attrib.get('title', '')
            post_url = a_tag.attrib.get('href', '')
            if post_url:
                full_post_url = "https://sxyprn.com" + post_url
            else:
                full_post_url = ''
            
            time_span = post.css_first('.post_control_time span')
            post_control_time = time_span.text.strip() if time_span else ''
            
            control_time_div = post.css_first('.post_control_time')
            if control_time_div:
                text_nodes = control_time_div.xpath('.//text()')
                text = ''.join(text_nodes).strip()
                if '·' in text:
                    parts = text.split('·')
                    if len(parts) > 1:
                        views_part = parts[1].strip()
                        views = views_part.split()[0] if views_part.split() else ''
                    else:
                        views = ''
                else:
                    views = ''
            else:
                views = ''
            
            data.append({
                'title': title,
                'post_url': full_post_url,
                'views': views,
                'post_control_time': post_control_time,
                'video_url': ''
            })
        except Exception as e:
            print(f"Error parsing post: {e}")
            continue
    return data

try:
    # Fetch the raw HTML from sxyprn.com using StealthyFetcher to bypass Cloudflare
    page = StealthyFetcher.fetch('https://sxyprn.com/Family-Therapy.html', headless=True, solve_cloudflare=True, network_idle=True, wait=3000)

    # Get the HTML content
    html_content = page.body

    # Parse the HTML
    parsed_page = Selector(html_content)

    # Parse pagination
    pagination_links = parsed_page.css('#center_control a')
    page_urls = []
    for link in pagination_links:
        href = link.attrib.get('href')
        if href:
            full_url = "https://sxyprn.com" + href
            page_urls.append(full_url)

    # Limit to 10 pages
    page_urls = page_urls[:10]

    # Parse current page first
    initial_data = parse_posts(html_content)

    # Fetch additional pages and video URLs using single session
    all_data = asyncio.run(fetch_all_data_with_videos(page_urls[1:], initial_data, max_tabs=5))

    # Save to JSON
    with open('posts_data.json', 'w', encoding='utf-8') as f:
        json.dump(all_data, f, indent=4, ensure_ascii=False)

    print(f"Extracted {len(all_data)} posts from {len(page_urls)} pages and saved to posts_data.json")

except Exception as e:
    print(f"Error: {e}")
    print("Unable to fetch data. Please check your connection or try again later.")

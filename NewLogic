from scrapling.fetchers import StealthyFetcher, AsyncStealthySession, DynamicFetcher
from scrapling.parser import Selector
import json
import asyncio
import re

def get_video_urls_dynamic(post_urls):
    """Extract video source URLs by monitoring network requests via DynamicFetcher.

    This clicks the player and listens to network responses to capture real media URLs.
    """
    video_urls = {}

    for url in post_urls:
        # Extract post ID for logging/filters (best-effort)
        m = re.search(r"/post/([a-f0-9]+)\.html", url)
        post_id = m.group(1) if m else None
        print(f"Processing {post_id or url} with DynamicFetcher...")

        captured = []

        def page_action(page):
            # Listen to network responses and capture media-like URLs
            def on_response(response):
                u = response.url
                if any(ext in u.lower() for ext in ('.vid', '.m3u8', '.mp4', '/videoplayback')):
                    if (post_id is None) or (post_id in u):
                        captured.append(u)
                        print(f"Captured media URL: {u}")

            page.on('response', on_response)

            # Wait basic load, then try to interact with likely player elements
            try:
                page.wait_for_load_state('domcontentloaded', timeout=15000)
            except Exception:
                pass

            selectors = [
                '#player_el', 'video', '.video-player', '[data-video]', '.player',
                '[class*="play"]', '[id*="video"]', '[id*="player"]'
            ]
            for sel in selectors:
                try:
                    el = page.query_selector(sel)
                    if el:
                        el.click()
                        page.wait_for_timeout(1500)
                        if captured:
                            break
                except Exception:
                    continue

            # Short grace period for late requests
            page.wait_for_timeout(1500)
            return page

        try:
            # Use StealthyFetcher here to bypass Cloudflare on individual post pages
            StealthyFetcher.fetch(
                url,
                headless=True,
                solve_cloudflare=True,
                network_idle=True,
                google_search=True,
                wait=1500,
                page_action=page_action,
                timeout=90000,
                disable_resources=False  # keep media allowed
            )
        except Exception as e:
            print(f"Error processing {url}: {e}")

        video_urls[url] = captured[0] if captured else ''

    return video_urls

async def fetch_and_parse_pages_async(page_urls, max_tabs=5):
    """
    Fetch and parse multiple pages concurrently using tab-based approach
    """
    all_data = []
    
    async with AsyncStealthySession(headless=True, solve_cloudflare=True) as session:
        # Create semaphore to limit concurrent tabs
        semaphore = asyncio.Semaphore(max_tabs)
        
        # Create tasks for all pages with semaphore control
        tasks = []
        for url in page_urls:
            tasks.append(fetch_single_page_with_semaphore(session, url, semaphore))
        
        # Execute all tasks concurrently
        print(f"Processing {len(page_urls)} pages with {max_tabs} concurrent tabs...")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for i, result in enumerate(results):
            url = page_urls[i]
            if isinstance(result, Exception):
                print(f"Error fetching {url}: {result}")
                continue
            else:
                all_data.extend(result)
                print(f"Fetched {len(result)} posts from {url}")
    
    return all_data

async def fetch_single_page_with_semaphore(session, url, semaphore):
    """
    Wrapper to control page fetching concurrency with semaphore
    """
    async with semaphore:
        return await fetch_single_page(session, url)

async def fetch_single_page(session, url):
    """
    Fetch and parse a single page using the existing session
    """
    try:
        # Add a small delay to avoid overwhelming the server
        await asyncio.sleep(0.2)
        page = await session.fetch(url)
        html = page.body
        return parse_posts(html)
    except Exception as e:
        raise e

def parse_posts(html_content):
    parsed_page = Selector(html_content)
    posts = parsed_page.css('.post_el_small .post_control')
    data = []
    for post in posts:
        try:
            a_tag = post.css_first('a.post_time')
            if not a_tag:
                continue
            title = a_tag.attrib.get('title', '')
            post_url = a_tag.attrib.get('href', '')
            if post_url:
                full_post_url = "https://sxyprn.com" + post_url
            else:
                full_post_url = ''
            
            time_span = post.css_first('.post_control_time span')
            post_control_time = time_span.text.strip() if time_span else ''
            
            control_time_div = post.css_first('.post_control_time')
            if control_time_div:
                text_nodes = control_time_div.xpath('.//text()')
                text = ''.join(text_nodes).strip()
                if '·' in text:
                    parts = text.split('·')
                    if len(parts) > 1:
                        views_part = parts[1].strip()
                        views = views_part.split()[0] if views_part.split() else ''
                    else:
                        views = ''
                else:
                    views = ''
            else:
                views = ''
            
            data.append({
                'title': title,
                'post_url': full_post_url,
                'views': views,
                'post_control_time': post_control_time,
                'video_url': ''
            })
        except Exception as e:
            print(f"Error parsing post: {e}")
            continue
    return data

try:
    # Fetch the raw HTML from sxyprn.com using StealthyFetcher to bypass Cloudflare
    # Avoid waiting for full 'load' to reduce timeouts; rely on domcontentloaded + small wait
    page = StealthyFetcher.fetch(
        'https://sxyprn.com/Fit18.html',
        headless=True,
        solve_cloudflare=True,
        google_search=True,
        timeout=90000,
        wait=1500
    )

    # Get the HTML content
    html_content = page.body

    # Parse the HTML
    parsed_page = Selector(html_content)

    # Parse pagination
    pagination_links = parsed_page.css('#center_control a')
    page_urls = []
    for link in pagination_links:
        href = link.attrib.get('href')
        if href:
            full_url = "https://sxyprn.com" + href
            page_urls.append(full_url)

    # Limit to 10 pages
    page_urls = page_urls[:10]

    # Parse current page first
    all_data = []
    all_data.extend(parse_posts(html_content))

    # Fetch and parse other pages concurrently
    if page_urls[1:]:  # If there are additional pages to fetch
        additional_data = asyncio.run(fetch_and_parse_pages_async(page_urls[1:], max_tabs=3))
        all_data.extend(additional_data)

    # Get video URLs using DynamicFetcher (network monitoring only)
    post_urls = [d['post_url'] for d in all_data if d['post_url']]
    if post_urls:
        print(f"Processing {len(post_urls)} video URLs with network monitoring...")
        video_urls = get_video_urls_dynamic(post_urls)
        for d in all_data:
            d['video_url'] = video_urls.get(d['post_url'], '')

    # Save to JSON
    with open('posts_data.json', 'w', encoding='utf-8') as f:
        json.dump(all_data, f, indent=4, ensure_ascii=False)

    print(f"Extracted {len(all_data)} posts from {len(page_urls)} pages and saved to posts_data.json")

except Exception as e:
    print(f"Error: {e}")
    print("Unable to fetch data. Please check your connection or try again later.")
